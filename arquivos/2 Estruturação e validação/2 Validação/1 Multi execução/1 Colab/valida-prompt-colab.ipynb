{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQXuRedyIshVDTWaICbnDI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"ueQAPmmUJSth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qt3DDWO1wTEP"},"source":["# Validação de prompt (multi execução)\n","Este notebook executa um mesmo prompt \"n\" vezes, para dois diferentes casos de busca, com o intuito de observar o comportamento do prompt em um cenário de variações (chats) e validar sua consistência (coerência e reprodutibilidade)."]},{"cell_type":"code","source":["# --------------------------------------------------\n","# Instalar dependências\n","# --------------------------------------------------\n","\n","# Instala a versão mais recente da OpenAI (se já estiver ok, pode pular esta linha)\n","!pip install --quiet --upgrade openai openpyxl pandas"],"metadata":{"id":"ijc1DtxZLfzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------------------------------------\n","# Algoritmo geral\n","# --------------------------------------------------\n","\n","# Importar bibliotecas\n","from openai import OpenAI\n","import os\n","import json\n","import re\n","import pandas as pd\n","\n","# COnfigurações gerais\n","NUM_RODADAS = 50  # <-- quantas execuções por caso\n","ARQ_SAIDA_XLSX = \"/content/resultados_obda.xlsx\"\n","\n","# ========= Chave de API OpenAI =========\n","\n","# Com variável de ambiente por segurança:\n","# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n","#client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n","\n","# ou\n","\n","# Com valor direto\n","client = OpenAI(api_key=\"\")\n","\n","# Casos de busca\n","casos_busca = [\n","    {\n","        \"nome\": \"Caso_1\",\n","        \"expressao\": \"\"\"\n","mostrar o \"id\" dos animais cuja espécie é da classe \"Giraffe\"\n","\"\"\".strip()\n","    },\n","    {\n","        \"nome\": \"Caso_2\",\n","        \"expressao\": \"\"\"\n","mostrar o \"id\" e a \"taxon\" dos animais cuja espécie é da classe \"Giraffe\"\n","\"\"\".strip()\n","    }\n","]\n","\n","# Carregar arquivos JSON\n","with open(\"/content/obda_chat_prompts_v3_PT-EN.json\", \"r\", encoding=\"utf-8\") as f:\n","    json_prompt = json.load(f)\n","\n","with open(\"/content/AnimalsDataBase_ADB_schema.json\", \"r\", encoding=\"utf-8\") as f:\n","    json_dados = json.load(f)\n","\n","with open(\"/content/AfricanWildlifeOntology_AWO.json\", \"r\", encoding=\"utf-8\") as f:\n","    json_ontologia = json.load(f)\n","\n","# Extrair contexto e regras\n","contexto_str = json_prompt[\"versions\"][\"pt-BR\"][\"main_prompt\"][\"context\"]\n","regras_str = \"\\n\".join(json_prompt[\"versions\"][\"pt-BR\"][\"main_prompt\"][\"rules\"][\"strict_rules\"])\n","\n","# Helper: parser dos 3 blocos\n","def extrair_tres_blocos(texto_resposta: str):\n","    \"\"\"\n","    Extrai (SQL, Ontop, SPARQL) nessa ordem, priorizando fenced code blocks (```).\n","    Possui fallback simples para não retornar vazio.\n","    \"\"\"\n","    # 1) Pegue os 3 primeiros blocos cercados por crases\n","    code_blocks = re.findall(r\"```[a-zA-Z0-9_\\-]*\\n([\\s\\S]*?)```\", texto_resposta)\n","    if len(code_blocks) >= 3:\n","        return code_blocks[0].strip(), code_blocks[1].strip(), code_blocks[2].strip()\n","\n","    # 2) Seções nomeadas + bloco cercado (mais tolerante)\n","    def cap(label_regex):\n","        m = re.search(rf\"{label_regex}[\\s\\S]*?```[a-zA-Z0-9_\\-]*\\n([\\s\\S]*?)```\",\n","                      texto_resposta, flags=re.IGNORECASE)\n","        return m.group(1).strip() if m else \"\"\n","\n","    sql    = cap(r\"(SQL\\s*Query|Consulta\\s*SQL)\")\n","    ontop  = cap(r\"(Ontop\\s*Mapping|Mapeamento\\s*Ontop)\")\n","    sparql = cap(r\"(SPARQL)\")\n","\n","    # 3) Fallback final: garante que não fiquem vazios\n","    if not (sql and ontop and sparql):\n","        partes = re.split(r\"\\n{2,}\", texto_resposta.strip())\n","        faltas = [k for k, ok in [(\"sql\", sql), (\"ontop\", ontop), (\"sparql\", sparql)] if not ok]\n","        for i, falta in enumerate(faltas):\n","            trecho = (partes[i].strip() if i < len(partes) else texto_resposta.strip())\n","            if falta == \"sql\" and not sql: sql = trecho\n","            elif falta == \"ontop\" and not ontop: ontop = trecho\n","            elif falta == \"sparql\" and not sparql: sparql = trecho\n","\n","    return sql, ontop, sparql\n","\n","# Execução: NUM_RODADAS por caso + XLSX (2 abas)\n","with pd.ExcelWriter(ARQ_SAIDA_XLSX, engine=\"openpyxl\") as writer:\n","    for caso in casos_busca:\n","        linhas = []\n","\n","        for rodada in range(NUM_RODADAS):\n","            # Monta as mensagens com o mesmo formato do seu código original\n","            messages = [\n","                {\"role\": \"system\", \"content\": contexto_str},\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"\"\"\n","Expressão de busca:\n","{caso['expressao']}\n","\n","Descrição da estrutura dos dados:\n","{json.dumps(json_dados, indent=2, ensure_ascii=False)}\n","\n","Descrição da ontologia:\n","{json.dumps(json_ontologia, indent=2, ensure_ascii=False)}\n","\n","Siga estritamente as seguintes regras:\n","{regras_str}\n","\n","Gere os três blocos de código: SQL Query, Ontop Mapping (.obda), SPARQL Query.\n","\"\"\".strip()\n","                }\n","            ]\n","\n","            # Chamada com parâmetros não determinísticos (ambiente de chats)\n","            response = client.chat.completions.create(\n","                model=\"gpt-4o\",\n","                messages=messages,\n","                temperature=0.7,\n","                top_p=0.9,\n","                frequency_penalty=0.3,\n","                presence_penalty=0.2,\n","                max_tokens=1500\n","            )\n","\n","            texto = (response.choices[0].message.content or \"\").strip()\n","            sql, ontop, sparql = extrair_tres_blocos(texto)\n","\n","            linhas.append({\n","                \"Rodada\": rodada + 1,\n","                \"SQL\": sql,\n","                \"Ontop Mapping\": ontop,\n","                \"SPARQL\": sparql\n","            })\n","\n","        # Salva a aba do caso atual\n","        df = pd.DataFrame(linhas, columns=[\"Rodada\", \"SQL\", \"Ontop Mapping\", \"SPARQL\"])\n","        df.to_excel(writer, index=False, sheet_name=caso[\"nome\"])\n","\n","print(f\"Arquivo gerado em: {ARQ_SAIDA_XLSX}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCkkpz9NMTOl","executionInfo":{"status":"ok","timestamp":1761228914792,"user_tz":180,"elapsed":596821,"user":{"displayName":"Cleiton R. M.","userId":"02298051794792469270"}},"outputId":"538d2e2f-4b82-4771-a4c5-f0d1ac87bfa9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Arquivo gerado em: /content/resultados_obda.xlsx\n"]}]}]}