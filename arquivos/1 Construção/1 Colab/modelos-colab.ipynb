{"cells":[{"cell_type":"markdown","metadata":{"id":"Qt3DDWO1wTEP"},"source":["# Execução de modelos (LLMs) no Google Colab via Hugging Face\n","Este notebook carrega os modelos **CodeLLaMA-7B** e **deepseek-coder-6.7b** via Hugging Face, com quantização em 8-bit para rodar na GPU gratuita do Colab. Após o carregamento, uma API é exposta para uso do modelo no ChainForge."],"id":"Qt3DDWO1wTEP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"install-deps"},"outputs":[],"source":["# --------------------------------------------------\n","# Instalar/atualizar dependências necessárias\n","# --------------------------------------------------\n","\n","# P. compatibilidade e estabilidade\n","!pip install torch transformers accelerate bitsandbytes safetensors\n","\n","# P. versões mais recentes\n","#!pip install -U bitsandbytes accelerate transformers einops safetensors\n"],"id":"install-deps"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aZVrYyvQn1b"},"outputs":[],"source":["# --------------------------------------------------\n","# Modelos\n","# --------------------------------------------------\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","\n","# Usados no projeto:\n","# - \"codellama/CodeLLaMA-7b-Instruct-hf\"\n","# - \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n","\n","MODEL_CHOICE = \"\"\n","\n","# Carregar tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_CHOICE)\n","\n","# Configuração para rodar em 4-bit (mais leve para Colab T4)\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","# Carregar modelo já na GPU (se disponível)\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_CHOICE,\n","    device_map=\"auto\",\n","    quantization_config=bnb_config\n",")\n","\n","# Criar pipeline direto\n","generator = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device_map=\"auto\",\n","\n","    # Parâmetros importantes -------------------------------------------------------------------------------\n","    temperature=0.0,                      # Garante saída determinística (sem aleatoriedade)\n","    top_p=1.0,                            # Considera 100% do vocabulário possível (sem corte)\n","    do_sample=False,                      # Desativa amostragem aleatória - usa sempre o token mais provável\n","    repetition_penalty=1.0                # Mantém repetições normais (ideal para código estruturado)\n","    # ------------------------------------------------------------------------------------------------------\n","\n","    #max_new_tokens=50,                   # limita a quantidade\n","    #eos_token_id=tokenizer.eos_token_id  # para encerrar a saída\n",")\n","\n","print(\"Modelo carregado e pronto para uso:\", MODEL_CHOICE)\n"],"id":"-aZVrYyvQn1b"},{"cell_type":"markdown","metadata":{"id":"3h1e4KoRwTEY"},"source":["## Exposição como API para uso no ChainForge"],"id":"3h1e4KoRwTEY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvJAwRliwTEY"},"outputs":[],"source":["# --------------------------------------------------\n","# Interface Gradio com link público\n","# --------------------------------------------------\n","\n","import gradio as gr\n","\n","def generate_code(prompt: str):\n","    try:\n","        out = generator(prompt, max_length=512, do_sample=True, temperature=0.0)\n","        if isinstance(out, list) and \"generated_text\" in out[0]:\n","            return out[0][\"generated_text\"]\n","        elif isinstance(out, list):\n","            return str(out[0])\n","        return str(out)\n","    except Exception as e:\n","        return f\"Erro na geração: {e}\"\n","\n","iface = gr.Interface(\n","    fn=generate_code,\n","    inputs=gr.Textbox(lines=8, placeholder=\"Digite o prompt para gerar código...\"),\n","    outputs=gr.Textbox(label=\"Código gerado\"),\n","    title=\"LLMs no Colab via Hugging Face\",\n","    description=\"Interface simples para testar prompts de geração de código\"\n",")\n","\n","# Criar link público temporário\n","iface.launch(share=True)"],"id":"YvJAwRliwTEY"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10"},"colab":{"provenance":[{"file_id":"1cw77cxL2ZjjOhTj_Fs6_hjMMrR6eiwRM","timestamp":1760615697225}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}